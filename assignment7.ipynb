{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with nltk\n",
    "def extract_information(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "        \n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    urls = []\n",
    "    emails = []\n",
    "    phone_numbers = []\n",
    "    hashtags = []\n",
    "    cleaned_text = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Extract URLs\n",
    "        urls.extend(re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', sentence))\n",
    "\n",
    "        # Extract email IDs\n",
    "        emails.extend(re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', sentence))\n",
    "\n",
    "        # Extract phone numbers\n",
    "        phone_numbers.extend(re.findall(r'\\b(?:\\+?(\\d{1,3}))?[-. (]*(\\d{3})[-. )]*(\\d{3})[-. ]*(\\d{4})\\b', sentence))\n",
    "\n",
    "        # Extract hashtags\n",
    "        hashtags.extend(re.findall(r'#\\w+', sentence))\n",
    "\n",
    "        # Remove repetitive characters\n",
    "        cleaned_text.append(re.sub(r'(.)\\1+', r'\\1', sentence))\n",
    "\n",
    "    return urls, emails, phone_numbers, hashtags, cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted URLs: ['http://www.johndoe.com']\n",
      "Extracted Email IDs: ['john.doe@example.com']\n",
      "Extracted Phone Numbers: [('1', '123', '456', '7890')]\n",
      "Extracted Hashtags: ['#mystery', '#fiction', '#bestseller']\n",
      "Cleaned Text: ['John Doe is an acomplished author, known for his best-seling novels like \"The Mystery of the Mising Manuscript\" and \"Secrets of the Silent Forest\".', 'You can visit his website at htp:/w.johndoe.com to learn more about his work.', 'For inquiries, you can reach out to him via email at john.doe@example.com or contact him directly at +1 (123) 456-7890.', 'Folow him on Twiter for updates using the hashtags #mystery #fiction #bestseler.', 'John is curently working on his next novel, which promises to be an exciting read.', 'He enjoys conecting with his readers and values their fedback.', 'Hey everyone, how are you?', \"Let's dive into the world of literature together!\"]\n"
     ]
    }
   ],
   "source": [
    "# Sample text\n",
    "file_path = 'book_author.txt'\n",
    "urls, emails, phone_numbers, hashtags, cleaned_text = extract_information(file_path)\n",
    "\n",
    "print(\"Extracted URLs:\", urls)\n",
    "print(\"Extracted Email IDs:\", emails)\n",
    "print(\"Extracted Phone Numbers:\", phone_numbers)\n",
    "print(\"Extracted Hashtags:\", hashtags)\n",
    "print(\"Cleaned Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted URLs: ['http://www.johndoe.com']\n",
      "Extracted Email IDs: ['john.doe@example.com']\n",
      "Extracted Phone Numbers: [('1', '123', '456', '7890')]\n",
      "Extracted Hashtags: ['#mystery', '#fiction', '#bestseller']\n",
      "Cleaned Text: John Doe is an acomplished author, known for his best-seling novels like \"The Mystery of the Mising Manuscript\" and \"Secrets of the Silent Forest\". You can visit his website at htp:/w.johndoe.com to learn more about his work. For inquiries, you can reach out to him via email at john.doe@example.com or contact him directly at +1 (123) 456-7890. Folow him on Twiter for updates using the hashtags #mystery #fiction #bestseler. John is curently working on his next novel, which promises to be an exciting read. He enjoys conecting with his readers and values their fedback. Hey everyone, how are you? Let's dive into the world of literature together!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#with re\n",
    "\n",
    "def extract_information(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    # Extract URLs\n",
    "    urls = re.findall(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', text)\n",
    "\n",
    "    # Extract email IDs\n",
    "    emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', text)\n",
    "\n",
    "    # Extract phone numbers\n",
    "    phone_numbers = re.findall(r'\\b(?:\\+?(\\d{1,3}))?[-. (]*(\\d{3})[-. )]*(\\d{3})[-. ]*(\\d{4})\\b', text)\n",
    "\n",
    "    # Extract hashtags\n",
    "    hashtags = re.findall(r'#\\w+', text)\n",
    "\n",
    "    # Remove repetitive characters\n",
    "    cleaned_text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "\n",
    "    return urls, emails, phone_numbers, hashtags, cleaned_text\n",
    "\n",
    "# Sample usage\n",
    "file_path = 'book_author.txt'\n",
    "urls, emails, phone_numbers, hashtags, cleaned_text = extract_information(file_path)\n",
    "\n",
    "print(\"Extracted URLs:\", urls)\n",
    "print(\"Extracted Email IDs:\", emails)\n",
    "print(\"Extracted Phone Numbers:\", phone_numbers)\n",
    "print(\"Extracted Hashtags:\", hashtags)\n",
    "print(\"Cleaned Text:\", cleaned_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
